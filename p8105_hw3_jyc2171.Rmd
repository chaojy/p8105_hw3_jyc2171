---
title: "Homework 3"
name: "Jerry Chao, Uni: jyc2171"
date: "October 4, 2020"
output: github_document
---

This is my solution to Homework 3

Problem 0

I have created....

```{r setup}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```

The instacart dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.  This is a big dataframe!  There appears to be 131,209 distinct user identification numbers.  There are 39,123  distinct product identification numbers corresponding to 39,123 product names, including "Im Pei-nut Butter" double chocolate cookie & peanut butter ice cream and #2 mechanical pencils as just two examples.  There are 134 aisle identification numbers corresponding to 134 aisles including air freshener candles, beer coolers, and baby food formula as some examples.  There are 21 department identification numbers corresponding to 21 departments, including alcohol, babies, and dry goods pasta as some examples.

Observations are the level of items in orders by user.  There are user / order variables -- user ID, order ID, order day, and order hour.  There are also item variables -- name, aisle, department, and some numeric codes.

How many aisles, and which are most items from?
(this is a question about counting)
Answer: There are 134 aisles and the top three aisles are (1) fresh vegetables, (2) fresh fruits, and (3) packaged vegetables fruits

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
*desc(n) from most to least

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(n)
```
*without the desc() function, lists from least to most

Let's make a plot
(this is a filtering problem)

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point()
```
*output is problematic

1) rotate axis labels.  google "rotate text"
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


2) a sensible way to arrange is to put from least number of products on left to most on right (x axis).  Key concept is to realize that should recode categorical variable (aisle) to factor.
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Let's make a table!!

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>%
  count(product_name) %>% 
  mutate(
    rank = min_rank(desc(n))
  ) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

Pink lady apples and coffee ice cream

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```


```{r}
instacart %>% 
  group_by(user_id) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(product_id) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(product_name) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(aisle, order_id) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(department) %>% 
  summarize(n_obs = n())

instacart %>% 
  count(department, name = "n_obs")

instacart %>% 
  count(order_number, name = "n_obs")

instacart %>% 
  group_by(aisle, order_number) %>% 
  relocate(aisle, order_number)
  mutate(order_number_rank = min_rank(desc(order_number))) %>% 
  relocate(aisle, order_number_rank, order_number)

  summarize(n_obs = n())
```

min_rank function gives you the lowest rank (#1) for the variable you specify

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  mutate(temp_rank = min_rank(desc(tmax))) %>% 
  filter(temp_rank == 1) %>% 
  view()
```
min_rank(desc(tmax)) allows you to calculate from the bottom up.

Problem 2

part 1
first part of question: importing, cleaning, and tidying data:
import the data fil
will need to convert to long form
mutate

part 2
if anything doesn't make sense, may need to refer back to part 1
aggregate.  pretty specific.  essentially a group_by and summarize() problem.
end up with table with 35 days and average measures of activity counts.  are there any trends.
goal here is after tidy, group by and summarize, and try to make sense of it.

part 3
if anything doesn't make sense, may need to refer back to part 1
first goal is show activity count (y axis) as function of minute (x axis)
from there, add geoms, start adding in some pieces (scatterplot with geom_plot).
color is day of week (aesthetic plot)
maybe plot is going to show something obvious?  activity level night vs day? activity level weekday vs. weekend?

two things that ar potential issues
part 2 table - day of week as column (may default to alphabetical order - so may have to convert to factor to order it in a meaningul way)
part 1 pivot long - watch out for when you tidy, make sure later on the plots are showing what you want to show.  watch out for variable classes when you tidy.

Problem 3
ny noaa dataset

part 1
separate() to make columns for different dates
check for reasonable units - tenths of degree C? might want to change to degrees C
for snowfall, this is a issue of count()

part 2
quite a bit there to unpack.
mostly a collection of data manipulation steps followed by a plot - how to i need to organize my data so I can have avg
group_by and summarize problem. ggplot thrown on top.
group by station, group by year, group by month, and then summarize to get what you want.  Only care about Jan and July - filter() fuunction somewhere - and probably early on.
then the plotting step - y axis avg max temp x axis month - repeated by station 1, 2, 3, etc

part 3
merging two separate plots (probably patchwork)
first one - scatterplot is bad b/c too many datapoints.  use hex plot, bin plot, etc.
second plot - first need filtering step.  then show a distribution: box plot, violin plot, ridge plot perhaps by year: 1981, 1982, 1983, etc.