---
title: "Homework 3"
name: "Jerry Chao, Uni: jyc2171"
date: "October 4, 2020"
output: github_document
---

This is my solution to Homework 3

Problem 0

I have created....

```{r setup}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```

My initial exploratory code:

```{r}
instacart %>% 
  group_by(user_id) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(product_id) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(product_name) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(aisle, order_id) %>% 
  summarize(n_obs = n())

instacart %>% 
  group_by(department) %>% 
  summarize(n_obs = n())

instacart %>% 
  count(department, name = "n_obs")

instacart %>% 
  count(order_number, name = "n_obs")

instacart %>% 
  group_by(aisle, order_number) %>% 
  relocate(aisle, order_number) %>% 
  mutate(order_number_rank = min_rank(desc(order_number))) %>% 
  relocate(aisle, order_number_rank, order_number) %>% 
  summarize(n_obs = n())
```


The instacart dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.  This is a big dataframe!  There appears to be 131,209 distinct user identification numbers.  There are 39,123  distinct product identification numbers corresponding to 39,123 product names, including "Im Pei-nut Butter" double chocolate cookie & peanut butter ice cream and #2 mechanical pencils as just two examples.  There are 134 aisle identification numbers corresponding to 134 aisles including air freshener candles, beer coolers, and baby food formula as some examples.  There are 21 department identification numbers corresponding to 21 departments, including alcohol, babies, and dry goods pasta as some examples.

Observations are the level of items in orders by user.  There are user / order variables -- user ID, order ID, order day, and order hour.  There are also item variables -- name, aisle, department, and some numeric codes.

How many aisles, and which are most items from?
(this is a question about counting)
Answer: There are 134 aisles and the top three aisles are (1) fresh vegetables, (2) fresh fruits, and (3) packaged vegetables fruits

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
*desc(n) from most to least

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(n)
```
*without the desc() function, lists from least to most

Let's make a plot
(this is a filtering problem)

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point()
```
*output is problematic

1) rotate axis labels.  google "rotate text"
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


2) a sensible way to arrange is to put from least number of products on left to most on right (x axis).  Key concept is to realize that should recode categorical variable (aisle) to factor.
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Let's make a table!!

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>%
  count(product_name) %>% 
  mutate(
    rank = min_rank(desc(n))
  ) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

Pink lady apples and coffee ice cream

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

Problem 2

part 1
first part of question: importing, cleaning, and tidying data:
import the data file
will need to convert to long form
mutate

```{r}
accel_df =
  read_csv("./data/accel_data.csv") %>% 
  pivot_longer(
    activity.1:activity.1440,
    names_to = "minute",
    names_prefix = "activity.",
    values_to = "activity"
  ) %>%
  mutate(
    weekend = recode(day, "Monday" = "weekday", "Tuesday" =  "weekday", "Wednesday" = "weekday",                      "Thursday" = "weekday", "Friday" = "weekday", "Saturday" = "weekend",                                           "Sunday" = "weekend"),
    minute = as.double(minute)
    ) %>% 
  rename(day_of_week = day)
  
```

The Accelerometer dataset contains five weeks of data from a single individual, a 63 year old nonobese male, who was admitted to Columbia University Medical Center for congestive heart failure.  The dataset contains activity data every minute of every day for 35 days (5 weeks).  The dataset consists of a week variable, day_id variable, a day of the week variable, minute variable, activity variable, and weekend variable, which indicates whether the day is a weekday or weekend.  The dataset is in long format with `r nrow(accel_df)` columns and `r ncol(accel_df)` columns.

part 2
if anything doesn't make sense, may need to refer back to part 1
aggregate.  pretty specific.  essentially a group_by and summarize() problem.
end up with table with 35 days and average measures of activity counts.  are there any trends.
goal here is after tidy, group by and summarize, and try to make sense of it.

```{r}
accel_df %>% 
  group_by(day_id, day_of_week, weekend) %>% 
  summarize(total_activity = sum(activity)) %>%
  knitr::kable()
```

On visual inspection of total activity by day, there appears to be a few days where total activity appears lower than usual.  These days are day 2 (a Monday), day 24 (a Saturday), day 31 (another Saturday).  The two Saturdays have the exact same activity level (1440), which corresponds to the number of minutes in a day.  It is unclear whether the accelerometers was placed on those Saturdays or if there was an equipment malfunction.  Otherwise, it is difficult to get a general trend without doing further analyses (e.g. descriptive statistics or plots)

part 3
if anything doesn't make sense, may need to refer back to part 1
first goal is show activity count (y axis) as function of minute (x axis)
from there, add geoms, start adding in some pieces (scatterplot with geom_plot).
color is day of week (aesthetic plot)
maybe plot is going to show something obvious?  activity level night vs day? activity level weekday vs. weekend?

two things that ar potential issues
part 2 table - day of week as column (may default to alphabetical order - so may have to convert to factor to order it in a meaningul way)
part 1 pivot long - watch out for when you tidy, make sure later on the plots are showing what you want to show.  watch out for variable classes when you tidy.

```{r}
accel_df %>% 
  ggplot(aes(x = minute, y = activity, color = day_of_week)) +
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE)

accel_df %>% 
  ggplot(aes(x = minute, y = activity, color = weekend)) +
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE)
```

In general, the scatterplot has many data points.  Activity is generally low from midnight to about 415 minutes (about 7 AM), at which point there is a general increase in activity, with some higher levels of activity, particularly on Thursdays.  On Fridays, there seems to also be increased activity after 500 minutes and on Sundays, increased activity later in the evening.  A quick plot of weekday vs. weekend activity could yield more information:

A scatterplot of weekday vs. weekend time periods shows increased activity earlier on weekdays, suggesting the patient wakes up earlier.  On weekends, activity increased begin later, suggesting he sleeps in.  On weekends, he has increased activity around 1000 minutes that isn't present on weekdays.  On weekdays, there seems to be another increase in activity at 1250 minutes (between 8 and 9 PM) greater than on weekends.  We can obtain additional information by taking a history from the patient.

Problem 3
ny noaa dataset

```{r}
library(p8105.datasets)
data("ny_noaa")
```

part 1
separate() to make columns for different dates
check for reasonable units - tenths of degree C? might want to change to degrees C
for snowfall, this is a issue of count()

```{r, eval = FALSE}
ny_noaa %>% 
  separate(date, into = c("year", "month", "day")) %>% 
  mutate(
    year = factor(year),
    month = factor(month),
    day = factor(day),
    tmax = as.integer(tmax),
    tmin = as.integer(tmin),
    prcp = prcp / 10,
    tmax = tmax / 10,
    tmin = tmin / 10,
    prcp = as.integer(prcp),
    tmax = as.integer(tmax),
    tmin = as.integer(tmin),
  ) 
  
ny_noaa %>%   
  count(snow) %>% 
  mutate(
    rank = min_rank(desc(n))
  ) %>% 
  filter(rank < 6) %>% 
  arrange(rank, snow) %>% 
  knitr::kable()

ny_noaa %>%   
  count(prcp) %>% 
  mutate(
    rank = min_rank(desc(n))
  ) %>% 
  filter(rank < 6) %>% 
  arrange(rank, prcp) %>% 
  knitr::kable()

ny_noaa %>%   
  count(snwd) %>% 
  mutate(
    rank = min_rank(desc(n))
  ) %>% 
  filter(rank < 6) %>% 
  arrange(rank, snwd) %>% 
  knitr::kable()

ny_noaa %>%   
  count(tmax) %>% 
  mutate(
    rank = min_rank(desc(n))
  ) %>% 
  filter(rank < 6) %>% 
  arrange(rank, tmax) %>% 
  knitr::kable()

ny_noaa %>%   
  count(tmin) %>% 
  mutate(
    rank = min_rank(desc(n))
  ) %>% 
  filter(rank < 6) %>% 
  arrange(rank, tmin) %>% 
  knitr::kable()
```

This is a large dataset from the National Oceanic and Atmospheric Association (NOAA) of the National Centers for Environmental Information (NCEI).  It contains data collected from all New York State weather stations with information about precipitation, snowfall, snow depth, maximum temperature, and minimum temperature.  I have recoded temperature to be in degrees Celsius and precipitation to be in millimeters.  The units for snowfall and snow depth remain unchanged in millimeters.  The dataset contains `r ncol(ny_noaa)` columns and `r nrow(ny_noaa)` rows.  It appears there is a lot of missing data.  For example, for snowfall, the most common observed values are "0" (77.4%) followed by "NA" (14.7%).  For both temperature variables (tmax and tmin), NA is the most common observation, by far (43.7% for both tmax and tmin variables).  For the precipitation and snow depth variables, the two most common observations are "0" followed by "NA".

part 2
quite a bit there to unpack.
mostly a collection of data manipulation steps followed by a plot - how to i need to organize my data so I can have avg
group_by and summarize problem. ggplot thrown on top.
group by station, group by year, group by month, and then summarize to get what you want.  Only care about Jan and July - filter() function somewhere - and probably early on.
then the plotting step - y axis avg max temp x axis month - repeated by station 1, 2, 3, etc

```{r}
set.seed(44)
ny_noaa %>% 
  sample_n(10000)
```



```{r, eval = FALSE}
ny_noaa %>% 
  separate(date, into = c("year", "month", "day")) %>% 
  mutate(
    year = factor(year),
    month = factor(month),
    day = factor(day),
    tmax = as.integer(tmax),
    tmin = as.integer(tmin),
    prcp = prcp / 10,
    tmax = tmax / 10,
    tmin = tmin / 10,
    prcp = as.integer(prcp),
    tmax = as.integer(tmax),
    tmin = as.integer(tmin),
  ) %>% 
  filter(month %in% c(1, 7)) %>%  
  group_by(month) %>% 
  summarize(tmax_mean = mean(tmax)) %>% 
  ggplot(aes(x = month, y = tmax_mean, color = month)) +
  geom_point()

```


part 3
merging two separate plots (probably patchwork)
first one - scatterplot is bad b/c too many datapoints.  use hex plot, bin plot, etc.
second plot - first need filtering step.  then show a distribution: box plot, violin plot, ridge plot perhaps by year: 1981, 1982, 1983, etc.